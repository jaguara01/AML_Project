---
title: "Your Compact Report"
author: "Your Name"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
fontsize: 10pt
geometry: "margin=0.75in"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)
library(caret)
library(ggcorrplot)
```

# Problem description and objectives

# Data description
The dataset used for this project is the Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC), a classic benchmark for classification sourced from the UCI Machine Learning Repository.

The dataset contains 569 observations of breast mass samples. The problem is a binary classification task, where the target variable, diagnosis, is classified as either Malignant (212 observations) or Benign (357 observations).

Each instance is described by 30 numeric features that were computed from a digitized image of a fine needle aspirate (FNA) of the breast mass.

```{r echo=FALSE}
# --- 1. Load Data ---
data_path <- "data/wdbc.data"
data_raw <- read.csv(data_path, header = FALSE)

# --- 2. Define and Assign Column Names ---
# As defined in 'wdbc.names'
col_names <- c(
  "id", "diagnosis",
  "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean",
  "radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se",
  "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst"
)

# --- 3. Assign these names to the dataframe ---
colnames(data_raw) <- col_names

# --- 4. Remove ID and create factors ---
data_clean <- data_raw %>%
  select(-id) %>%
  mutate(diagnosis = as.factor(diagnosis))
```


# Pre-processing

## Missing values
```{r pressure, echo=FALSE}
# --- Check for Total Missing Values ---
missing_vals_total <- sum(is.na(data_clean))
cat("Total missing values in the entire dataset:", missing_vals_total)
```

## Outliers, features distributions - scales
```{r}
# To plot all 30 features at once, we first "pivot" the data
# into a "long" format. This is the standard tidyverse way.
data_long <- data_clean %>%
  pivot_longer(
    cols = -diagnosis,       # Pivot every column *except* 'diagnosis'
    names_to = "feature",    # New column for the feature name
    values_to = "value"      # New column for its value
  )

ggplot(data_long, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~feature, scales = "free_y", ncol = 5) +
  scale_fill_brewer(palette = "Paired") +
  labs(
    title = "Feature Distributions by Diagnosis (Malignant vs. Benign)",
    x = "Diagnosis",
    y = "Feature Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),  # Hide x-axis labels for a cleaner look
    axis.ticks.x = element_blank(),
    strip.text = element_text(size = 7) # Smaller facet labels
  )
```

* **Outliers (Anomalous Value):** 
We immediately observed the presence of numerous outliers, represented by individual observations outside the IQR range. These are particularly present in features like `area_mean`, `area_worst`, and `concavity_worst`. 
However, because those trust the original source, we know that these are not data entry errors but likely represent true, extreme biological values. Therefore, we made the decision not to remove these outliers, as they are part of the real-world problem. We will proceed with the full dataset, relying on our models to be robust enough to handle this variance.

## Normalization

Our plot also highlights the vast difference in scales across features. Gradient-based models like the Perceptron and Logistic Regression (GLM) are sensitive to feature scales. Without normalization, features with larger magnitudes would dominate the learning process. This plot provides a clear justification for standardizing the features before modeling.

## Elimination of irrelevant variables

For nearly every feature, the boxplot for the Malignant (M) class is visually distinct from the boxplot for the Benign (B) class. The medians, quartiles, and overall ranges show clear separation. This is an excellent sign, as it indicates that our features contain strong predictive information, and we can expect our models to perform well.

```{r}
# With AI
nzv_check <- nearZeroVar(data_clean, saveMetrics = TRUE)

# Filter to see only the features that *might* be a problem
# A "TRUE" in the 'nzv' column means it's a candidate for removal.
problematic_features <- nzv_check[nzv_check$nzv == TRUE, ]

# Print the results
if (nrow(problematic_features) > 0) {
  cat("Found", nrow(problematic_features), "near-zero variance features:\n")
  print(problematic_features)
} else {
  cat("No near-zero variance features found. All features have sufficient variance.\n")
}
```

First, we checked for "irrelevant" features by running a "Near-Zero Variance" (NZV) test. This test finds any feature that is constant or nearly constant, as a feature that doesn't change cannot be a predictor. We used the `caret::nearZeroVar()` function to check all 30 features.

we note that we used AI to help confirm this was the correct tool for this task.

The NZV test showed that all 30 features have enough variance, so we did not eliminate any features at this step.

## Elimination of redundant variables

Our primary tool for this step was correlation analysis. We first calculated the correlation matrix for all numeric features and visualized it using a heatmap.

```{r}
# Correlation matrix.
# numeric features selections
numeric_features <- data_clean %>%
  select_if(is.numeric)

cor_matrix <- cor(numeric_features)

# Plot the correlation heatmap
ggcorrplot(cor_matrix,
  type = "lower",
  lab = FALSE,
  colors = c("#2166AC", "white", "#B2182B"),
  title = "Correlation Heatmap of 30 Numeric Features"
)
```
The heatmap immediately revealed large blocks of high correlation (dark red), confirming that our dataset contains significant multicollinearity. For example, features related to tumor size, like radius_mean, perimeter_mean, and area_mean, are all nearly perfectly correlated.
```{r}
# Correlation and cutoff

highly_correlated_features <- findCorrelation(cor_matrix, cutoff = 0.90, names = TRUE)


cat("Found", length(highly_correlated_features), "redundant features to remove (cutoff > 0.90):\n")
print(highly_correlated_features)


# Create the reduced Dataset

data_clean_reduced <- data_clean %>%
  select(-all_of(highly_correlated_features))

cat("\nOriginal data had", ncol(data_clean), "columns (30 features + 1 target).\n")
cat("Reduced data has", ncol(data_clean_reduced), "columns.\n")
```
While the heatmap is excellent for visualization, we use a correlation matrix and a cutoff of 0.90 (standard threshold) for reducing feature redundancy. This function automatically identifies the minimum number of features to remove to ensure that no two features in the remaining dataset have a correlation greater than 0.90.

This analysis identified 10 features as redundant.

## Normalization of the variables
done in models

# Models

```{r warning=FALSE, echo=FALSE}
# --- 6. Final Data Splitting ---
# We will use our *reduced 20-feature dataset* as the starting point.

set.seed(123) # for reproducibility
train_index <- createDataPartition(data_clean_reduced$diagnosis, p = 0.80, list = FALSE)

# These are now our main training/testing dataframes
training_data <- data_clean_reduced[train_index, ]
holdout_data  <- data_clean_reduced[-train_index, ] # For final testing later


# --- 7. Model Training (3x3 Experiment) ---
# We will use 10-fold cross-validation
cv_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final" # To collect results for t-tests
)

# --- 7.1. EXPERIMENT 1: Models on 20 Reduced Features (Baseline) ---
# preProcess = c("center", "scale")
cat("Starting Experiment 1: 20 Reduced Features\n")

model_glm_reduced <- train(
  diagnosis ~ ., data = training_data, method = "glm",
  trControl = cv_control, preProcess = c("center", "scale")
)

model_gnb_reduced <- train(
  diagnosis ~ ., data = training_data, method = "nb", # 'nb' is Naive Bayes
  trControl = cv_control, preProcess = c("center", "scale")
)

model_svm_reduced <- train(
  diagnosis ~ ., data = training_data, method = "svmLinear",
  trControl = cv_control, preProcess = c("center", "scale")
)

# --- 7.2. EXPERIMENT 2: Models on PCA Components ---
# preProcess = c("center", "scale", "pca")
cat("Starting Experiment 2: PCA Components\n")

model_glm_pca <- train(
  diagnosis ~ ., data = training_data, method = "glm",
  trControl = cv_control, preProcess = c("center", "scale", "pca")
)

model_gnb_pca <- train(
  diagnosis ~ ., data = training_data, method = "nb",
  trControl = cv_control, preProcess = c("center", "scale", "pca")
)

model_svm_pca <- train(
  diagnosis ~ ., data = training_data, method = "svmLinear",
  trControl = cv_control, preProcess = c("center", "scale", "pca")
)

# --- 7.3. EXPERIMENT 3: Models on LDA Component ---
cat("Starting Experiment 3: LDA Component (2-Step Method)\n")

# --- Step 1: Train the LDA model itself ---
# This serves as our "LDA model" for comparison AND as our transformer
# We use the 20-feature 'training_data'
cat("Training LDA model (to be used as transformer)...\n")
model_lda_reduced <- train(
  diagnosis ~ .,
  data = training_data, # 20-feature 'training_data'
  method = "lda",
  trControl = cv_control, # This is our 10-fold CV
  preProcess = c("center", "scale")
)

# --- Step 2: Create the new, single-feature datasets ---
# We use our trained LDA model to "predict" and extract the new component.
# This creates new dataframes with just 'diagnosis' and 'LD1'.
cat("Extracting LD1 component...\n")
train_data_lda_comp <- data.frame(
  diagnosis = training_data$diagnosis,
  LD1 = predict(model_lda_reduced, newdata = training_data)
)

# We also transform our holdout data for the final test
holdout_data_lda_comp <- data.frame(
  diagnosis = holdout_data$diagnosis,
  LD1 = predict(model_lda_reduced, newdata = holdout_data)
)

# --- Step 3: Train GLM, GNB, and SVM on the new 'LD1' feature ---
# We must use a *new* cv_control because the data is different
# (e.g., we don't need to center/scale a single feature).
cv_control_lda <- trainControl(method = "cv", number = 10, savePredictions = "final")

cat("Training GLM on LDA component...\n")
model_glm_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "glm",
  trControl = cv_control_lda
)

cat("Training GNB on LDA component...\n")
model_gnb_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "nb",
  trControl = cv_control_lda
)

cat("Training SVM on LDA component...\n")
model_svm_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "svmLinear",
  trControl = cv_control_lda
)

cat("All LDA-component models trained.\n")
```

## Final results
```{r}
# --- 8. Final Results Comparison (Corrected) ---
# We now collect all our models.
all_model_results <- resamples(list(
  # Experiment 1 (Reduced 20 Features)
  GLM_Reduced = model_glm_reduced,
  GNB_Reduced = model_gnb_reduced,
  SVM_Reduced = model_svm_reduced,
  LDA_Reduced = model_lda_reduced,
  
  # Experiment 2 (PCA Components)
  GLM_PCA     = model_glm_pca,
  GNB_PCA     = model_gnb_pca,
  SVM_PCA     = model_svm_pca,

  # Experiment 3 (LDA Component)
  GLM_LDA_Comp = model_glm_lda,
  GNB_LDA_Comp = model_gnb_lda,
  SVM_LDA_Comp = model_svm_lda
))

# 1. Show the summary table
cat("Final CV Performance Summary:\n")
summary(all_model_results)

# 2. Plot the results
dotplot(all_model_results, metric = "Accuracy")

# 3. Statistical Comparison (as requested by teacher)
cat("\nFinal Statistical Comparison (Paired t-test):\n")
all_model_diffs <- diff(all_model_results)
summary(all_model_diffs)
```

```{r}
all_model_results$timings
```


## Accuracy vs. Training Time Trade-off
```{r}
# --- 9. Accuracy vs. Training Time Trade-off (FIXED) ---

# We need 'tibble' to use rownames_to_column
library(tibble) 

# --- Process Accuracy Data ---
# This part was correct, as '$values' *is* per-fold data
accuracy_stats <- all_model_results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = "ModelMetric",
    values_to = "Value"
  ) %>%
  tidyr::separate(ModelMetric, into = c("Model", "Metric"), sep = "~") %>%
  filter(Metric == "Accuracy") %>%
  group_by(Model) %>%
  summarize(Mean_Accuracy = mean(Value, na.rm = TRUE))


# --- Process Timings Data (This is the FIX) ---
# As your screenshot shows, '$timings' is a SUMMARY table.
# The 'Model' is in the row names, and 'Everything' is the total time.
time_stats <- all_model_results$timings %>%
  
  # 1. Convert row names (e.g., "GLM_Reduced") into a real column
  rownames_to_column(var = "Model") %>%
  
  # 2. Select the Model and the 'Everything' column (total time)
  #    and rename 'Everything' to match our other table.
  select(Model, Mean_Time_sec = Everything)


# --- Join and Plot ---
# This join will now work because both tables have a 'Model' column
performance_summary <- inner_join(accuracy_stats, time_stats, by = "Model")

# 4. Print the final summary table
cat("--- Final Performance vs. Cost Summary ---\n")
performance_summary <- performance_summary %>%
  arrange(desc(Mean_Accuracy))
print(performance_summary)

# 5. Plot the Trade-off
ggplot(performance_summary, aes(x = Mean_Accuracy, y = Mean_Time_sec, color = Model)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_text(aes(label = Model), vjust = -1.2, size = 3.5) + 
  labs(
    title = "Model Performance vs. Training Time Trade-off",
    subtitle = "Best models are in the top-left corner (High Accuracy, Low Time)",
    x = "Mean Accuracy (10-fold CV)",
    y = "Mean Training Time (seconds)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
# --- 10. Analysis of Prediction Time per Fold ---
#
# While 'resamples()' only gives us *total* training time,
# the individual 'train' objects store the *prediction time* for each fold.
# This is a key measure of a model's efficiency.

# 1. Create a new dataframe from the per-fold prediction timings
#    (We'll use our stable 7-model set)
pred_times_df <- data.frame(
  Fold = 1:10, # Assuming 10 folds
  
  # Experiment 1
  GLM_Reduced = model_glm_reduced$times$prediction,
  GNB_Reduced = model_gnb_reduced$times$prediction,
  SVM_Reduced = model_svm_reduced$times$prediction,
  LDA_Reduced = model_lda_reduced$times$prediction,
  
  # Experiment 2
  GLM_PCA     = model_glm_pca$times$prediction,
  GNB_PCA     = model_gnb_pca$times$prediction,
  SVM_PCA     = model_svm_pca,
  
  # Experiment 3
  GLM_LDA_Comp = model_glm_lda$times$prediction
)

# 2. Pivot this new dataframe into a long format for plotting
pred_times_long <- pred_times_df %>%
  pivot_longer(
    cols = -Fold,
    names_to = "Model",
    values_to = "PredictionTime_sec"
  )

# 3. Create a boxplot to compare the distribution of prediction times
ggplot(pred_times_long, aes(x = Model, y = PredictionTime_sec, fill = Model)) +
  geom_boxplot() +
  labs(
    title = "Model Prediction Time per Fold (10-fold CV)",
    subtitle = "A measure of model efficiency at prediction time. Lower is better.",
    x = "Model",
    y = "Prediction Time (seconds)"
  ) +
  scale_y_log10() + # Use a log scale, as SVM might be much slower
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"
  )
```


