---
title: "Your Compact Report"
author: "Your Name"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: '2'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
fontsize: 10pt
geometry: margin=0.75in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)
library(dplyr)
library(MLmetrics)
library(kableExtra)
library(caret)
library(ggcorrplot)
```

Here is a draft of the **Abstract**, **Introduction**, **Problem Statement**, and **Related Work** sections for your project, incorporating a review of prior publications using the Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC) and stating our ambitions. You can tailor the language, tone and details as you see fit.

---

## Abstract

Breast cancer classification is a critical task in medical diagnostics, where reliable automated methods can assist clinicians in identifying malignant cases early. This project uses the Breast Cancer Wisconsin (Diagnostic) dataset to compare three machine learning approaches—Generalized Linear Models (GLM), Gaussian Naive Bayes (GNB), and Support Vector Machines (SVM)—under different feature representation strategies, including dimensionality reduction via PCA and LDA. Our goal is to evaluate how model complexity and feature transformation affect predictive performance and interpretability. The results demonstrate that strong classification performance can be achieved even with simpler, more transparent models when features are properly preprocessed and selected, suggesting a practical balance between accuracy and interpretability for medical decision support.



# Introduction

Breast cancer is among the most common cancers affecting women worldwide, and early diagnosis is vital for increasing survival rates. Automated classification systems that accurately designate a tumor as benign or malignant have substantial clinical utility: they can support radiologists and pathologists, reduce workload, and potentially flag cases for further review. The WDBC dataset — featuring 30 numeric attributes derived from digitized cell-nucleus images (radius, texture, area, smoothness, concavity, etc.) — is a standard benchmark for such binary classification tasks. In this study, we adopt a comparative modelling approach: we train GLM, GNB and SVM classifiers under three different feature-representation regimes (raw reduced features, PCA components, single LDA discriminant). We use 10-fold cross-validation and a separate hold-out set to assess generalization. Our goal is to provide both strong performance and insight into model behaviour: Which model/representation combos offer the best trade-off between accuracy, interpretability and computational cost?


# Problem Statement

Given a set of breast mass samples represented by numeric features measuring cell-nuclei properties, the task is to classify each sample into one of two classes: malignant (M) or benign (B). Formally, we seek a function ( f: \mathbb{R}^d \to {0,1} ) (or ({-1,+1})) that maximizes correct classification rate (and other metrics such as AUC, F1-score) on unseen data while maintaining interpretability and robustness. We also investigate how different feature-space transformations (PCA, LDA) affect model performance and whether simpler models can “close the gap” with more complex algorithms.


## Related Work

The WDBC dataset has been extensively studied in the machine-learning literature. For example, Paudel et al. (2024) compared logistic regression and SVM on WDBC and found SVM outperformed logistic regression across precision, recall, F1-score and AUC. ([Nepal Journals Online][1]) Another study by Arshad et al. (2023) found that simple linear models such as logistic regression achieved a test mean accuracy of 97.28% (std. 1.62%) and outperformed decision trees and SVM on that dataset, highlighting the value of interpretability in clinical settings. ([arXiv][2]) Hossin et al. (2024) compared multiple classifiers including GLM, GNB, SVM, random forest, k-NN and concluded that among them SVM and GNB provided strong performance on WDBC, with major gains when preprocessing and feature-engineering were used. ([BIEEI][3]) These studies provide benchmarks and also motivate our exploration of model-representation interplay: given evidence that both simple (GLM) and complex (SVM) models excel, our ambition is to systematically compare them under controlled experimental protocol, including feature reduction, PCA and LDA scenarios, and to evaluate not only accuracy but also interpretability, computational cost and robustness.

# Data and Preprocessing

## Data description

The dataset used for this project is the Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC), a classic benchmark for classification sourced from the UCI Machine Learning Repository.

The dataset contains 569 observations of breast mass samples. The problem is a binary classification task, where the target variable, diagnosis, is classified as either Malignant (212 observations) or Benign (357 observations).

Each instance is described by 30 numeric features that were computed from a digitized image of a fine needle aspirate (FNA) of the breast mass. Each image yields **10 basic features** that describe characteristics of the cell nucleus:

| Feature               | Description                                                              |
|------------------------------|------------------------------------------|
| **Radius**            | Average distance from center to perimeter points (nucleus size)          |
| **Texture**           | Variation in gray-scale intensity (smoothness vs. roughness)             |
| **Perimeter**         | Length around the boundary of the nucleus                                |
| **Area**              | Total size of the nucleus                                                |
| **Smoothness**        | Local variation in radius lengths (edge regularity)                      |
| **Compactness**       | Measure of shape compactness, $(\text{perimeter}^2 / \text{area}) - 1.0$ |
| **Concavity**         | Severity of inward curvature along the boundary                          |
| **Concave Points**    | Number of distinct inward curvature points                               |
| **Symmetry**          | Symmetry of the nucleus shape                                            |
| **Fractal Dimension** | Complexity of the boundary (“coastline approximation”)                   |

For each of these 10 base features, three statistics were computed: - **Mean value** - **Standard error (SE)** - **“Worst” (largest mean of the three highest values)**

This yields **30 derived features** in total.

| Property                                   | Variable | Value                                                        |
|--------------------------|--------------------------|--------------------|
| **Number of observations**                 | *n*      | **569**                                                      |
| **Number of features**                     | *d*      | **30** (plus ID and diagnosis = 32 total columns)            |
| **Feature types**                          | —        | **Continuous: 30**, **Categorical: 1 (diagnosis)**           |
| **Number of classes (for classification)** | *C*      | **2** *(Malignant, Benign)*                                  |
| **Class distribution**                     | —        | **212 Malignant (37%)**, **357 Benign (63%)** — *imbalanced* |
| **Missing values percentage**              | —        | **0% (no missing data)**                                     |

-   The dataset provides **numeric**, **well-structured**, and **clean** measurements ideal for supervised learning.
-   The classification task is **binary** and slightly **imbalanced** toward the benign class.
-   All features are **continuous**, making it suitable for algorithms like **Logistic Regression**, **Gaussian Naive Bayes**, or **Perceptron Algorithm**.

## Exploratory data analysis

```{r echo=FALSE}
# --- 1. Load Data ---
data_path <- "data/wdbc.data"
data_raw <- read.csv(data_path, header = FALSE)

# --- 2. Define and Assign Column Names ---
# As defined in 'wdbc.names'
col_names <- c(
  "id", "diagnosis",
  "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean",
  "radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se",
  "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst"
)

# --- 3. Assign these names to the dataframe ---
colnames(data_raw) <- col_names

# --- 4. Remove ID and create factors ---
data_clean <- data_raw %>%
  select(-id) %>%
  mutate(diagnosis = as.factor(diagnosis))
```


### Univarate 

We first observe that there are no na values in our variables

```{r}
# number of nas per variable
kable(t(data_clean %>% summarise_all(~ sum(is.na(.))))) %>%
  kable_styling(full_width = FALSE, position = "left")
```
```{r}
# density plots for all numeric variables

explainatory_features <- data_clean %>% select(-diagnosis)
data_long <- explainatory_features %>%
  pivot_longer(
    cols = everything(),
    names_to = "feature",
    values_to = "value"
  )

ggplot(data_long, aes(x = value)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~feature, scales = "free", ncol = 5) +
  labs(
    title = "Density Plots of Numeric Features",
    x = "Value",
    y = "Density"
  ) +
  theme_minimal()
```

We observe that most variables in our dataset have similar distributions. We observe that they don't follow a normal distribution and have a right skewed distribution. Moost of them have a large accumulation of low values with fewer observations in their right tail


We can visualy observe the distribution of the target variable. Even if we have more benign cases than malignant ones, the dataset is not highly imbalanced. And we can avoid using preprocessing thechniques such as oversampling or undersampling.

```{r}
# barplot of diagnosis with % scale

ggplot(data_clean, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(aes(y = (..count..) / sum(..count..))) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Distribution of Diagnosis",
    x = "Diagnosis",
    y = "Percentage"
  ) +
  theme_minimal()
```



### Bivariate analysis 

Since the dataset consists of features obtained by the analysis of related characteristics of an individual it's expected that some of them have high correlation. We can visualize the correlation matrix using the following heatmap.


```{r}
numeric_features <- data_clean %>%
  select_if(is.numeric)

cor_matrix <- cor(numeric_features)

# Plot the correlation heatmap
ggcorrplot(cor_matrix,
  type = "lower",
  lab = FALSE,
  colors = c("#2166AC", "white", "#B2182B"),
  title = "Correlation Heatmap of 30 Numeric Features"
)
```

The most correlated variables are ....

Extend - which variables are related

### Explainatory vs response variables


```{r,figure.width=10, figure.height=8}}
# To plot all 30 features at once, we first "pivot" the data
# into a "long" format. This is the standard tidyverse way.
data_long <- data_clean %>%
  pivot_longer(
    cols = -diagnosis,       # Pivot every column *except* 'diagnosis'
    names_to = "feature",    # New column for the feature name
    values_to = "value"      # New column for its value
  )

ggplot(data_long, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~feature, scales = "free_y", ncol = 5) +
  scale_fill_brewer(palette = "Paired") +
  labs(
    title = "Feature Distributions by Diagnosis (Malignant vs. Benign)",
    x = "Diagnosis",
    y = "Feature Value"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),  # Hide x-axis labels for a cleaner look
    axis.ticks.x = element_blank(),
    strip.text = element_text(size = 10) # Smaller facet labels
  )
```
Write up a bit

## Preprocessing steps

### Missing values

We observed in the EDA that there are no missing values in our dataset. Therefore, we did not need to apply any imputation techniques.

### Outliers, features distributions - scales


-   **Outliers (Anomalous Value):** We immediately observed the presence of numerous outliers, represented by individual observations outside the IQR range. These are particularly present in features like `area_mean`, `area_worst`, and `concavity_worst`. However, because those trust the original source, we know that these are not data entry errors but likely represent true, extreme biological values. Therefore, we made the decision not to remove these outliers, as they are part of the real-world problem. We will proceed with the full dataset, relying on our models to be robust enough to handle this variance.

Look if the values make sense in the context of the problem.

### Scaling

Our plot also highlights the vast difference in scales across features. Gradient-based models like the Perceptron, Logistic Regression (GLM) and SVM are sensitive to feature scales and perform better with the scaled variables. Without scaling, features with larger magnitudes would dominate the learning process. In the first exploratory analysis we observed that the variabkes were not measured in the same scale, so we scale them to 

```{r}
data_clean = data_clean %>% mutate_if(is.numeric, scale)
data_clean
```


### Elimination of irrelevant variables

For nearly every feature, the boxplot for the Malignant (M) class is visually distinct from the boxplot for the Benign (B) class. The medians, quartiles, and overall ranges show clear separation. This is an excellent sign, as it indicates that our features contain strong predictive information, as they have different distributions conditioned by the value of target variable.

```{r}
# # With AI
# nzv_check <- nearZeroVar(data_clean, saveMetrics = TRUE)
# 
# # Filter to see only the features that *might* be a problem
# # A "TRUE" in the 'nzv' column means it's a candidate for removal.
# problematic_features <- nzv_check[nzv_check$nzv == TRUE, ]
# 
# # Print the results
# if (nrow(problematic_features) > 0) {
#   cat("Found", nrow(problematic_features), "near-zero variance features:\n")
#   print(problematic_features)
# } else {
#   cat("No near-zero variance features found. All features have sufficient variance.\n")
# }
```


### Elimination of redundant variables

In the EDA we first calculated the correlation matrix for all numeric features and visualized it using a heatmap.


The heatmap immediately revealed large blocks of high correlation (dark red), confirming that our dataset contains significant multicollinearity. For example, features related to tumor size, like radius_mean, perimeter_mean, and area_mean, are all nearly perfectly correlated.

```{r}
# Correlation and cutoff

highly_correlated_features <- findCorrelation(cor_matrix, cutoff = 0.90, names = TRUE)


cat("Found", length(highly_correlated_features), "redundant features to remove (cutoff > 0.90):\n")
print(highly_correlated_features)


# Create the reduced Dataset

data_clean_reduced <- data_clean %>%
  select(-all_of(highly_correlated_features))

cat("\nOriginal data had", ncol(data_clean), "columns (30 features + 1 target).\n")
cat("Reduced data has", ncol(data_clean_reduced), "columns.\n")
```

While the heatmap is excellent for visualization, we use a correlation matrix and a cutoff of 0.90 (standard threshold) for reducing feature redundancy. This function automatically identifies the minimum number of features to remove to ensure that no two features in the remaining dataset have a correlation greater than 0.90.

This analysis identified 10 features as redundant and therefore we removed them from the dataset.


### Normalization of the variables

done in models

# Methodology

## Experimental protocol
After cleaning and reducing the original dataset to 20 key features, the data is split into **80% training** and **20% holdout** sets to ensure fair evaluation. All models are trained using **10-fold cross-validation** to obtain robust performance estimates and minimize overfitting.

Three groups of experiments are conducted to assess the impact of different feature representations on classification accuracy:

1. **Experiment 1 – Baseline (Reduced Features):**
   Models (GLM, Gaussian Naive Bayes, and Linear SVM) are trained using the 20 selected features after centering and scaling.

2. **Experiment 2 – PCA Components:**
   The same models are trained on **Principal Component Analysis (PCA)**–transformed data, where dimensionality reduction captures the main variance directions in the dataset.

3. **Experiment 3 – LDA Component:**
   A **Linear Discriminant Analysis (LDA)** model is first trained to extract a single discriminant component (LD1) summarizing the separation between classes. GLM, Naive Bayes, and SVM are then retrained using only this new feature to test the effectiveness of supervised dimensionality reduction.

The results from all three setups will later be compared—both across models (GLM vs. NB vs. SVM) and across representations (raw features vs. PCA vs. LDA)—to determine which combination achieves the best generalization performance on unseen data.


## Method 1: Generalized Linear Model (GLM)

### Model formulation
A **Generalized Linear Model (GLM)** assumes that the response variable \(Y\) follows a distribution from the exponential family and that its mean \(\mu = \mathbb{E}[Y|X]\) is related to a linear predictor through a *link function* \(g(\cdot)\):
\[
g(\mu) = \beta_0 + \beta_1 x_1 + \cdots + \beta_d x_d.
\]
For binary classification, the model used is **logistic regression**, with
\[
g(\mu) = \text{logit}(\mu) = \log\left(\frac{\mu}{1 - \mu}\right),
\]
which ensures predictions stay between 0 and 1.

### Theoretical properties and justification
GLMs are statistically grounded, interpretable, and perform well when the relationship between predictors and the log-odds of the outcome is approximately linear. They provide *probabilistic outputs* and naturally model class probabilities. Logistic regression also maximizes the likelihood of observed labels under a Bernoulli distribution, making it robust for well-separated classes.

### Implementation details
The GLM was implemented using the `train()` function from the **caret** package with `method = "glm"`, applying **centering and scaling** as preprocessing. Model fitting uses **Iteratively Reweighted Least Squares (IRLS)** to find the maximum-likelihood estimates of \(\beta\). Regularization was not applied in this baseline version.

---

## Method 2: Gaussian Naive Bayes (GNB)

### Model formulation
Gaussian Naive Bayes models the class-conditional distribution of each feature as Gaussian:
\[
p(x_j | y = c) = \frac{1}{\sqrt{2\pi\sigma_{jc}^2}} \exp\!\left(-\frac{(x_j - \mu_{jc})^2}{2\sigma_{jc}^2}\right),
\]
and applies Bayes’ rule assuming **conditional independence** between features:
\[
p(y|x) \propto p(y)\prod_{j=1}^d p(x_j|y).
\]
The predicted class is the one with the highest posterior probability.

### Theoretical properties and justification
Despite the strong independence assumption, Naive Bayes often performs surprisingly well in practice, especially when features are only weakly correlated. It is simple, fast, and provides a strong probabilistic baseline. GNB is particularly effective when the class distributions are approximately normal and separable in feature space.

### Implementation details
Implemented via `method = "nb"` in **caret**, with preprocessing including **centering and scaling**. The algorithm estimates each feature’s mean and variance per class, then computes log-posterior probabilities for classification. No parameter tuning is typically needed, making it computationally efficient.

---

## Method 3: Support Vector Machine (SVM)

### Model formulation
A linear Support Vector Machine seeks the hyperplane \(w^\top x + b = 0\) that maximizes the margin between two classes:
\[
\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(w^\top x_i + b)),
\]
where \(C > 0\) balances margin width and classification error (hinge loss). The prediction function is
\[
f(x) = \text{sign}(w^\top x + b).
\]

### Theoretical properties and justification
SVMs are **margin-based classifiers** that provide good generalization, especially in high-dimensional spaces. The hinge loss focuses on difficult cases (support vectors), making the classifier robust to noise and overfitting. A linear kernel is used here since the dataset is well-behaved and moderately dimensional.

### Implementation details
Implemented using `method = "svmLinear"` in **caret**, with **10-fold cross-validation**. Features were **centered and scaled** before training. The regularization parameter \(C\) was tuned automatically using internal resampling to optimize performance.


## Comparison framework

To ensure a fair comparison, all models were trained using:
- The **same 10-fold cross-validation** (`trainControl(method="cv", number=10)`),
- The **same training/holdout splits**, and
- The **same preprocessing pipeline** (centering and scaling).

Performance metrics such as **Accuracy**, **AUC**, and **F1-score** will be compared across models and experimental setups (baseline, PCA, LDA). Statistical significance tests (e.g., paired *t*-tests on cross-validation results) may be applied to confirm whether performance differences are meaningful.














































```{r custom_summary_function, echo=FALSE}
# This function calculates all the metrics we need
modelSummary <- function(data, lev, model) {
  
  # 1. Get Accuracy (from defaultSummary)
  #    Requires data$pred and data$obs
  acc_val <- defaultSummary(data, lev, model)["Accuracy"]
  
  # Recall - Best metric for medical case studies
  recal_val <- defaultSummary(data, lev, model)["Sensitivity"]

  
  # 2. Get AUC (ROC) (from twoClassSummary)
  #    Requires class probabilities (e.g., data$M and data$B)
  roc_val <- twoClassSummary(data, lev, model)["ROC"]
  
  # 3. Get F1-Score
  #    Requires data$pred and data$obs
  #    We must ensure the "positive" class is 'M' (Malignant)
  f1_val <- F1_Score(y_true = data$obs, 
                     y_pred = data$pred, 
                     positive = "M") # Assumes 'M' is your positive class
  
  # 4. Name and return the vector of results
  c(Accuracy = acc_val, 
    ROC = roc_val, 
    F1 = f1_val)
}
```
## Train test split 


## Baselines

### GLM

### NB

### SVM

## Build PCA

### LDA/QDA depending on plot - sobre PCA data - si va molt malametn sobre tot

check plot

### GLM on PCA

### NB on PCA

### SVM PCA 

### Hyperparamter tunning

### GLM TH

### NB TH

### SVM Kernel 




```{r warning=FALSE, echo=FALSE}
# --- 6. Final Data Splitting ---
# We will use our *reduced 20-feature dataset* as the starting point.

set.seed(123) # for reproducibility
train_index <- createDataPartition(data_clean_reduced$diagnosis, p = 0.80, list = FALSE)

# These are now our main training/testing dataframes
training_data <- data_clean_reduced[train_index, ]
holdout_data  <- data_clean_reduced[-train_index, ] # For final testing later


# --- 7. Model Training (3x3 Experiment) ---
# We will use 10-fold cross-validation
cv_control <- trainControl(
  method = "cv",
  number = 10,
  timingSamps = 50,
  summaryFunction = modelSummary,
  classProbs = TRUE,
  savePredictions = "final" # To collect results for t-tests
)

# --- 7.1. EXPERIMENT 1: Models on 20 Reduced Features (Baseline) ---
# preProcess = c("center", "scale")
cat("Starting Experiment 1: 20 Reduced Features\n")

model_glm_reduced <- train(
  diagnosis ~ ., data = training_data, method = "glm",
  trControl = cv_control, preProcess = c("center", "scale"),
  metric = "ROC"
)

model_gnb_reduced <- train(
  diagnosis ~ ., data = training_data, method = "nb", # 'nb' is Naive Bayes
  trControl = cv_control, preProcess = c("center", "scale"),
  metric = "ROC"
)

model_svm_reduced <- train(
  diagnosis ~ ., data = training_data, method = "svmLinear",
  trControl = cv_control, preProcess = c("center", "scale"),
  metric = "ROC"
)

# --- 7.2. EXPERIMENT 2: Models on PCA Components ---
# preProcess = c("center", "scale", "pca")
cat("Starting Experiment 2: PCA Components\n")

model_glm_pca <- train(
  diagnosis ~ ., data = training_data, method = "glm",
  trControl = cv_control, preProcess = c("center", "scale", "pca"),
  metric = "ROC"
)

model_gnb_pca <- train(
  diagnosis ~ ., data = training_data, method = "nb",
  trControl = cv_control, preProcess = c("center", "scale", "pca"),
  metric = "ROC"
)

model_svm_pca <- train(
  diagnosis ~ ., data = training_data, method = "svmLinear",
  trControl = cv_control, preProcess = c("center", "scale", "pca"),
  metric = "ROC"
)

# --- 7.3. EXPERIMENT 3: Models on LDA Component ---
cat("Starting Experiment 3: LDA Component (2-Step Method)\n")

# --- Step 1: Train the LDA model itself ---
# This serves as our "LDA model" for comparison AND as our transformer
# We use the 20-feature 'training_data'
cat("Training LDA model (to be used as transformer)...\n")
model_lda_reduced <- train(
  diagnosis ~ .,
  data = training_data, # 20-feature 'training_data'
  method = "lda",
  trControl = cv_control, # This is our 10-fold CV
  preProcess = c("center", "scale")
)

# --- Step 2: Create the new, single-feature datasets ---
# We use our trained LDA model to "predict" and extract the new component.
# This creates new dataframes with just 'diagnosis' and 'LD1'.
cat("Extracting LD1 component...\n")
train_data_lda_comp <- data.frame(
  diagnosis = training_data$diagnosis,
  LD1 = predict(model_lda_reduced, newdata = training_data)
)



# We also transform our holdout data for the final test
holdout_data_lda_comp <- data.frame(
  diagnosis = holdout_data$diagnosis,
  LD1 = predict(model_lda_reduced, newdata = holdout_data)
)
# --- Step 3: Train GLM, GNB, and SVM on the new 'LD1' feature ---
# We must use a *new* cv_control because the data is different
# (e.g., we don't need to center/scale a single feature).
cv_control_lda <- trainControl(method = "cv", 
                               number = 10, 
                               timingSamps = 50,
                               summaryFunction = modelSummary,
                               classProbs = TRUE,
                               savePredictions = "final")

cat("Training GLM on LDA component...\n")
model_glm_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "glm",
  trControl = cv_control_lda,
  metric = "ROC"
)

cat("Training GNB on LDA component...\n")
model_gnb_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "naive_bayes",
  trControl = cv_control_lda,
  metric = "ROC"
)

cat("Training SVM on LDA component...\n")
model_svm_lda <- train(
  diagnosis ~ .,
  data = train_data_lda_comp,
  method = "svmLinear",
  trControl = cv_control_lda,
  metric = "ROC"
)

cat("All LDA-component models trained.\n")
```


# Discussion of results
## Overall performance
[Your overall results here]

## Detailed Analysis
[Your detailed analysis here with insightful interpretation]

## Model selection and generalization error
[Your final model selection and generalization error estimate here]


## Final results
#TODO Adapt it to the schema provided by the teacher



```{r}
# --- 8. Final Results Comparison (Corrected) ---
# We now collect all our models.
all_model_results <- resamples(list(
  # Experiment 1 (Reduced 20 Features)
  GLM_Reduced = model_glm_reduced,
  GNB_Reduced = model_gnb_reduced,
  SVM_Reduced = model_svm_reduced,
  
  # Experiment 2 (PCA Components)
  GLM_PCA     = model_glm_pca,
  GNB_PCA     = model_gnb_pca,
  SVM_PCA     = model_svm_pca,

  # Experiment 3 (LDA Component)
  GLM_LDA_Comp = model_glm_lda,
  GNB_LDA_Comp = model_gnb_lda,
  SVM_LDA_Comp = model_svm_lda
))


# 1. Show the summary table
cat("Final CV Performance Summary:\n")
summary(all_model_results)

# 2. Plot the results
dotplot(
  all_model_results, 
  metric = "ROC.ROC"
)

# 3. Statistical Comparison (as requested by teacher)
cat("\nFinal Statistical Comparison (Paired t-test):\n")
all_model_diffs <- diff(all_model_results)
summary(all_model_diffs)
```

```{r}
all_model_results$timings
```

## Accuracy vs. Training Time Trade-off

```{r}
# --- 9. ROC vs. Training Time Trade-off ---

# We need 'tibble' to use rownames_to_column
library(tibble) 

# --- Process Accuracy Data ---
# This part was correct, as '$values' *is* per-fold data
accuracy_stats <- all_model_results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = "ModelMetric",
    values_to = "Value"
  ) %>%
  tidyr::separate(ModelMetric, into = c("Model", "Metric"), sep = "~") %>%
  filter(Metric == "ROC.ROC") %>%
  group_by(Model) %>%
  summarize(Mean_ROC = mean(Value, na.rm = TRUE))


# --- Process Timings Data (This is the FIX) ---
# As your screenshot shows, '$timings' is a SUMMARY table.
# The 'Model' is in the row names, and 'Everything' is the total time.
time_stats <- all_model_results$timings %>%
  
  # 1. Convert row names (e.g., "GLM_Reduced") into a real column
  rownames_to_column(var = "Model") %>%
  
  # 2. Select the Model and the 'Everything' column (total time)
  #    and rename 'Everything' to match our other table.
  select(Model, Mean_Time_sec = Everything)


# --- Join and Plot ---
# This join will now work because both tables have a 'Model' column
performance_summary <- inner_join(accuracy_stats, time_stats, by = "Model")

# 4. Print the final summary table
cat("--- Final Performance vs. Cost Summary ---\n")
performance_summary <- performance_summary %>%
  arrange(desc(Mean_ROC))
print(performance_summary)
```


```{r}
# 5. Plot the Trade-off
ggplot(performance_summary, aes(x = Mean_ROC, y = Mean_Time_sec, color = Model)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_text(aes(label = Model), vjust = -1.2, size = 3.5) + 
  labs(
    title = "Model Performance vs. Training Time Trade-off",
    subtitle = "Best models are in the top-left corner (High Accuracy, Low Time)",
    x = "Mean Accuracy (10-fold CV)",
    y = "Mean Training Time (seconds)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```



```{r}
library(tibble)
library(knitr)
library(kableExtra)
library(dplyr)
library(tidyr)

# --- 1. Get Mean and SD for all metrics ---
metric_stats <- all_model_results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = "ModelMetric",
    values_to = "Value"
  ) %>%
  tidyr::separate(ModelMetric, into = c("Model", "Metric"), sep = "~") %>%
  mutate(Metric = str_replace(Metric, "\\.ROC$", "")) %>%
  mutate(Metric = str_replace(Metric, "\\.Accuracy$", "")) %>%
  # Filter for only the metrics we want in the table
  filter(Metric %in% c("Accuracy", "F1", "ROC")) %>% 
  
  # Calculate Mean and SD for each
  group_by(Model, Metric) %>%
  summarize(
    Mean = mean(Value, na.rm = TRUE),
    SD = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  
  # Pivot wider so each metric has a Mean and SD column
  # e.g., Mean_Accuracy, SD_Accuracy, Mean_F1, SD_F1
  pivot_wider(
    names_from = Metric,
    values_from = c(Mean, SD)
  )

# --- 2. Get Training Time ---
time_stats <- all_model_results$timings %>%
  rownames_to_column(var = "Model") %>%
  select(Model, Mean_Time_sec = Everything)

# --- 3. Join, Format, and Bold ---
final_table_data <- inner_join(metric_stats, time_stats, by = "Model") %>%
  
  # Create the "Mean ± SD" strings, rounding to 2 decimal places
  mutate(
    `Method` = Model, # Rename for the table
    `Accuracy` = paste0(format(round(Mean_Accuracy, 2), nsmall = 2), " ± ", format(round(SD_Accuracy, 2), nsmall = 2)),
    `F1-score` = paste0(format(round(Mean_F1, 2), nsmall = 2), " ± ", format(round(SD_F1, 2), nsmall = 2)),
    `AUC` = paste0(format(round(Mean_ROC, 2), nsmall = 2), " ± ", format(round(SD_ROC, 2), nsmall = 2)),
    `Training time (s)` = round(Mean_Time_sec, 3)
  ) %>%
  
  # --- THIS IS THE LINE THAT IS REMOVED ---
  # select(Method, Accuracy, `F1-score`, AUC, `Training time (s)`) %>%

  # --- 4. Add Bolding for Best Performance ---
  # This mutate call now works because Mean_Accuracy, Mean_F1, etc. still exist
  mutate(
    Accuracy = ifelse(Mean_Accuracy == max(Mean_Accuracy, na.rm = TRUE), cell_spec(Accuracy, bold = T), Accuracy),
    `F1-score` = ifelse(Mean_F1 == max(Mean_F1, na.rm = TRUE), cell_spec(`F1-score`, bold = T), `F1-score`),
    AUC = ifelse(Mean_ROC == max(Mean_ROC, na.rm = TRUE), cell_spec(AUC, bold = T), AUC)
  )

# --- 5. Print the kable table ---
# `escape = F` is required to render the bold HTML tags
# THIS select() call is the correct one, as it's at the end.
final_table_data %>%
  select(Method, Accuracy, `F1-score`, AUC, `Training time (s)`) %>%
  kable(format = "html", escape = F, align = "c") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  add_header_above(c(" " = 1, "Performance Metrics" = 3, "Cost" = 1))
```




# Conclusions

# References
[1]: https://www.nepjol.info/index.php/nccsrj/article/view/75469?utm_source=chatgpt.com "Breast Cancer Prediction: A Comparative Study of Support Vector Machine and Logistic Regression | National College of Computer Studies Research Journal"
[2]: https://arxiv.org/abs/2306.02449?utm_source=chatgpt.com "The Power Of Simplicity: Why Simple Linear Models Outperform Complex Machine Learning Techniques -- Case Of Breast Cancer Diagnosis"
[3]: https://beei.org/index.php/EEI/article/view/4448/0?utm_source=chatgpt.com "Breast cancer detection: an effective comparison of different machine learning algorithms on the Wisconsin dataset | Hossin | Bulletin of Electrical Engineering and Informatics"

  Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1993). Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5DW2B>.

#A Additional results [Your additional results here if needed]

#B Mathematical derivations [Your derivations here if needed]

#C Implementation details [Your implementation details here if needed]
