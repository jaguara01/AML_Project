---
title: "Your Compact Report"
author: "Your Name"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: '2'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    latex_engine: xelatex
fontsize: 10pt
geometry: margin=0.75in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)
library(dplyr)
library(corrplot)
library(MLmetrics)
library(robustbase)
library(kableExtra)
library(caret)
library(ggcorrplot)
library(dplyr)
library(glmnet)      
library(pROC)        
library(e1071)       
library(klaR)        
```


## Abstract

Breast cancer classification is a critical task in medical diagnostics, where reliable automated methods can assist clinicians in identifying malignant cases early. This project uses the Breast Cancer Wisconsin (Diagnostic) dataset to compare three machine learning approaches—Generalized Linear Models (GLM), Gaussian Naive Bayes (GNB), and Support Vector Machines (SVM)—under different feature representation strategies, including dimensionality reduction via PCA and LDA. Our goal is to evaluate how model complexity and feature transformation affect predictive performance and interpretability. The results demonstrate that strong classification performance can be achieved even with simpler, more transparent models when features are properly preprocessed and selected, suggesting a practical balance between accuracy and interpretability for medical decision support.



# Introduction

Breast cancer is among the most common cancers affecting women worldwide, and early diagnosis is vital for increasing survival rates. Automated classification systems that accurately designate a tumor as benign or malignant have substantial clinical utility: they can support radiologists and pathologists, reduce workload, and potentially flag cases for further review. The WDBC dataset — featuring 30 numeric attributes derived from digitized cell-nucleus images (radius, texture, area, smoothness, concavity, etc.) — is a standard benchmark for such binary classification tasks. In this study, we adopt a comparative modelling approach: we train GLM, GNB and SVM classifiers under three different feature-representation regimes (raw reduced features, PCA components, single LDA discriminant). We use 10-fold cross-validation and a separate hold-out set to assess generalization. Our goal is to provide both strong performance and insight into model behaviour: Which model/representation combos offer the best trade-off between accuracy, interpretability and computational cost?


# Problem Statement

Given a set of breast mass samples represented by numeric features measuring cell-nuclei properties, the task is to classify each sample into one of two classes: malignant (M) or benign (B). Formally, we seek a function ( f: \mathbb{R}^d \to {0,1} ) (or ({-1,+1})) that maximizes correct classification rate (and other metrics such as AUC, F1-score) on unseen data while maintaining interpretability and robustness. We also investigate how different feature-space transformations (PCA, LDA) affect model performance and whether simpler models can “close the gap” with more complex algorithms.


## Related Work

The WDBC dataset has been extensively studied in the machine-learning literature. For example, Paudel et al. (2024) compared logistic regression and SVM on WDBC and found SVM outperformed logistic regression across precision, recall, F1-score and AUC. ([Nepal Journals Online][1]) Another study by Arshad et al. (2023) found that simple linear models such as logistic regression achieved a test mean accuracy of 97.28% (std. 1.62%) and outperformed decision trees and SVM on that dataset, highlighting the value of interpretability in clinical settings. ([arXiv][2]) Hossin et al. (2024) compared multiple classifiers including GLM, GNB, SVM, random forest, k-NN and concluded that among them SVM and GNB provided strong performance on WDBC, with major gains when preprocessing and feature-engineering were used. ([BIEEI][3]) These studies provide benchmarks and also motivate our exploration of model-representation interplay: given evidence that both simple (GLM) and complex (SVM) models excel, our ambition is to systematically compare them under controlled experimental protocol, including feature reduction, PCA and LDA scenarios, and to evaluate not only accuracy but also interpretability, computational cost and robustness.

# Data and Preprocessing

## Data description

The dataset used for this project is the Breast Cancer Wisconsin (Diagnostic) Dataset (WDBC), a classic benchmark for classification sourced from the UCI Machine Learning Repository.

The dataset contains 569 observations of breast mass samples. The problem is a binary classification task, where the target variable, diagnosis, is classified as either Malignant (212 observations) or Benign (357 observations).

Each instance is described by 30 numeric features that were computed from a digitized image of a fine needle aspirate (FNA) of the breast mass. Each image yields **10 basic features** that describe characteristics of the cell nucleus:

| Feature               | Description                                                              |
|------------------------------|------------------------------------------|
| **Radius**            | Average distance from center to perimeter points (nucleus size)          |
| **Texture**           | Variation in gray-scale intensity (smoothness vs. roughness)             |
| **Perimeter**         | Length around the boundary of the nucleus                                |
| **Area**              | Total size of the nucleus                                                |
| **Smoothness**        | Local variation in radius lengths (edge regularity)                      |
| **Compactness**       | Measure of shape compactness, $(\text{perimeter}^2 / \text{area}) - 1.0$ |
| **Concavity**         | Severity of inward curvature along the boundary                          |
| **Concave Points**    | Number of distinct inward curvature points                               |
| **Symmetry**          | Symmetry of the nucleus shape                                            |
| **Fractal Dimension** | Complexity of the boundary (“coastline approximation”)                   |

For each of these 10 base features, three statistics were computed: - **Mean value** - **Standard error (SE)** - **“Worst” (largest mean of the three highest values)**

This yields **30 derived features** in total.

| Property                                   | Variable | Value                                                        |
|--------------------------|--------------------------|--------------------|
| **Number of observations**                 | *n*      | **569**                                                      |
| **Number of features**                     | *d*      | **30** (plus ID and diagnosis = 32 total columns)            |
| **Feature types**                          | —        | **Continuous: 30**, **Categorical: 1 (diagnosis)**           |
| **Number of classes (for classification)** | *C*      | **2** *(Malignant, Benign)*                                  |
| **Class distribution**                     | —        | **212 Malignant (37%)**, **357 Benign (63%)** — *imbalanced* |
| **Missing values percentage**              | —        | **0% (no missing data)**                                     |

-   The dataset provides **numeric**, **well-structured**, and **clean** measurements ideal for supervised learning.
-   The classification task is **binary** and slightly **imbalanced** toward the benign class.
-   All features are **continuous**, making it suitable for algorithms like **Logistic Regression**, **Gaussian Naive Bayes**, or **Perceptron Algorithm**.

## Exploratory data analysis

```{r echo=FALSE}
# --- 1. Load Data ---
data_path <- "data/wdbc.data"
data_raw <- read.csv(data_path, header = FALSE)

# --- 2. Define and Assign Column Names ---
# As defined in 'wdbc.names'
col_names <- c(
  "id", "diagnosis",
  "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave_points_mean", "symmetry_mean", "fractal_dimension_mean",
  "radius_se", "texture_se", "perimeter_se", "area_se", "smoothness_se", "compactness_se", "concavity_se", "concave_points_se", "symmetry_se", "fractal_dimension_se",
  "radius_worst", "texture_worst", "perimeter_worst", "area_worst", "smoothness_worst", "compactness_worst", "concavity_worst", "concave_points_worst", "symmetry_worst", "fractal_dimension_worst"
)

# --- 3. Assign these names to the dataframe ---
colnames(data_raw) <- col_names

# --- 4. Remove ID and create factors ---
data_clean <- data_raw %>%
  dplyr::select(-id) %>% 
  dplyr::mutate(diagnosis = as.factor(diagnosis))
```


### Univarate 

We first observe that there are no na values in our variables

```{r}
# number of nas per variable
data_clean %>%
  summarise_all(~ sum(is.na(.))) %>%
  pivot_longer(everything(),names_to = "variable",values_to = "NA count") %>%
  kable() %>%
  kable_styling(full_width = FALSE, position = "left")
```


```{r}
# density plots for all numeric variables

explainatory_features <- data_clean %>% dplyr::select(-diagnosis)
data_long <- explainatory_features %>% 
  pivot_longer(cols = everything(),names_to = "feature",values_to = "value")

ggplot(data_long, aes(x = value)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~feature, scales = "free", ncol = 5) +
  labs(title = "Density Plots of Numeric Features",x = "Value",y = "Density") +
  theme_minimal()
```

We observe that most variables in our dataset have similar distributions. We observe that they don't follow a normal distribution and have a right skewed distribution. Moost of them have a large accumulation of low values with fewer observations in their right tail


We can visualy observe the distribution of the target variable. Even if we have more benign cases than malignant ones, the dataset is not highly imbalanced. And we can avoid using preprocessing thechniques such as oversampling or undersampling.

```{r}
# barplot of diagnosis with % scale

ggplot(data_clean, aes(x = diagnosis, fill = diagnosis)) +
  geom_bar(aes(y = (..count..) / sum(..count..))) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Distribution of Diagnosis",x = "Diagnosis",y = "Percentage") +
  theme_minimal()
```



### Bivariate analysis 

Since the dataset consists of features obtained by the analysis of related characteristics of an individual it's expected that some of them have high correlation. We can visualize the correlation matrix using the following heatmap.


```{r}
numeric_features <- data_clean %>% select_if(is.numeric)

cor_matrix <- cor(numeric_features)

#ggcorrplot(cor_matrix,type = "lower",lab = FALSE,colors = c("#2166AC", "white", "#B2182B"),title = "Correlation Heatmap of 30 Numeric Features")

corrplot(cor_matrix,type = "lower",  tl.col = "black",title = "Pearson Correlation Heatmap of 30 Numeric Features")

```

The most correlated variables are ....

Extend - which variables are related

### Explainatory vs response variables


```{r,figure.width=10, figure.height=8}}
# To plot all 30 features at once, we first "pivot" the data
# into a "long" format. This is the standard tidyverse way.
data_long <- data_clean %>%
  pivot_longer(cols = -diagnosis,names_to = "feature",values_to = "value")

ggplot(data_long, aes(x = diagnosis, y = value, fill = diagnosis)) +
  geom_boxplot() +
  facet_wrap(~feature, scales = "free_y", ncol = 5) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Feature Distributions by Diagnosis (Malignant vs. Benign)",x = "Diagnosis",y = "Feature Value") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),axis.ticks.x = element_blank(),strip.text = element_text(size = 10) )
```
Write up a bit

## Preprocessing steps

### Missing values

We observed in the EDA that there are no missing values in our dataset. Therefore, we did not need to apply any imputation techniques.

### Outliers, features distributions - scales


-   **Outliers (Anomalous Value):** We immediately observed the presence of numerous outliers, represented by individual observations outside the IQR range. These are particularly present in features like `area_mean`, `area_worst`, and `concavity_worst`. However, because those trust the original source, we know that these are not data entry errors but likely represent true, extreme biological values. Therefore, we made the decision not to remove these outliers, as they are part of the real-world problem. We will proceed with the full dataset, relying on our models to be robust enough to handle this variance.

Look if the values make sense in the context of the problem.

### Scaling

Our plot also highlights the vast difference in scales across features. Gradient-based models like the Perceptron, Logistic Regression (GLM) and SVM are sensitive to feature scales and perform better with the scaled variables. Without scaling, features with larger magnitudes would dominate the learning process. In the first exploratory analysis we observed that the variabkes were not measured in the same scale, so we scale them to 

```{r}
data_clean <- data_clean %>% mutate_if(is.numeric, ~ as.numeric(scale(.)))
```


### Elimination of irrelevant variables

For nearly every feature, the boxplot for the Malignant (M) class is visually distinct from the boxplot for the Benign (B) class. The medians, quartiles, and overall ranges show clear separation. This is an excellent sign, as it indicates that our features contain strong predictive information, as they have different distributions conditioned by the value of target variable.

```{r}
# # With AI
# nzv_check <- nearZeroVar(data_clean, saveMetrics = TRUE)
# 
# # Filter to see only the features that *might* be a problem
# # A "TRUE" in the 'nzv' column means it's a candidate for removal.
# problematic_features <- nzv_check[nzv_check$nzv == TRUE, ]
# 
# # Print the results
# if (nrow(problematic_features) > 0) {
#   cat("Found", nrow(problematic_features), "near-zero variance features:\n")
#   print(problematic_features)
# } else {
#   cat("No near-zero variance features found. All features have sufficient variance.\n")
# }
```


### Elimination of redundant variables

In the EDA we first calculated the correlation matrix for all numeric features and visualized it using a heatmap.


The heatmap immediately revealed large blocks of high correlation (dark red), confirming that our dataset contains significant multicollinearity. For example, features related to tumor size, like radius_mean, perimeter_mean, and area_mean, are all nearly perfectly correlated.

```{r}
# Correlation and cutoff

highly_correlated_features <- findCorrelation(cor_matrix, cutoff = 0.90, names = TRUE)


cat("Found", length(highly_correlated_features), "redundant features to remove (cutoff > 0.90):\n")
print(highly_correlated_features)


# Create the reduced Dataset

data_clean_reduced <- data_clean %>%
  dplyr::select(-all_of(highly_correlated_features))

cat("\nOriginal data had", ncol(data_clean), "columns (30 features + 1 target).\n")
cat("Reduced data has", ncol(data_clean_reduced), "columns.\n")
```

While the heatmap is excellent for visualization, we use a correlation matrix and a cutoff of 0.90 (standard threshold) for reducing feature redundancy. This function automatically identifies the minimum number of features to remove to ensure that no two features in the remaining dataset have a correlation greater than 0.90.

This analysis identified 10 features as redundant and therefore we removed them from the dataset.


### Normalization of the variables

done in models

# Methodology

## Experimental protocol
After cleaning the dataset and reducing it to the 20 most informative features, the data is split into **80% training** and **20% holdout** sets to ensure fair and unbiased evaluation. All models are trained using **10-fold cross-validation**, which provides robust performance estimates and helps reduce overfitting by averaging results across multiple folds.

Three experimental stages are conducted to study how different feature representations and model configurations influence classification performance:

1. **Experiment 1 – Baseline (Reduced Features):**
   GLM, Gaussian Naive Bayes, and Linear SVM are trained using the 20 selected features after applying standard preprocessing (centering and scaling).

2. **Experiment 2 – PCA Representation:**
   The same models are trained on data transformed through **Principal Component Analysis (PCA)**. This experiment evaluates whether unsupervised dimensionality reduction improves generalization by capturing the main directions of variance.

3. **Experiment 3 – Model-specific Hyperparameter Tuning:**
   Each model undergoes a dedicated hyperparameter optimization procedure (e.g., adjusting the regularization parameter (C) for SVM, or lambda in Logistic Regression). This experiment aims to identify the best-performing configuration for each classifier.

The performance of all experiments will later be compared—both across models (GLM vs. GNB vs. SVM) and across the two feature representations (original vs. PCA)—to determine which approach achieves the strongest generalization on the unseen holdout set.



## Method 1: Generalized Linear Model (GLM)

### Model formulation
A **Generalized Linear Model (GLM)** assumes that the response variable \(Y\) follows a distribution from the exponential family and that its mean \(\mu = \mathbb{E}[Y|X]\) is related to a linear predictor through a *link function* \(g(\cdot)\):
\[
g(\mu) = \beta_0 + \beta_1 x_1 + \cdots + \beta_d x_d.
\]
For binary classification, the model used is **logistic regression**, with
\[
g(\mu) = \text{logit}(\mu) = \log\left(\frac{\mu}{1 - \mu}\right),
\]
which ensures predictions stay between 0 and 1.

### Theoretical properties and justification
GLMs are statistically grounded, interpretable, and perform well when the relationship between predictors and the log-odds of the outcome is approximately linear. They provide *probabilistic outputs* and naturally model class probabilities. Logistic regression also maximizes the likelihood of observed labels under a Bernoulli distribution, making it robust for well-separated classes.


### Implementation details

The GLM is trained using the `train()` function from the **caret** package (`method = "glm"`). By default, this corresponds to **unregularized logistic regression**, where the coefficients (\beta) are estimated by maximizing the log-likelihood via **Iteratively Reweighted Least Squares (IRLS)**.

For the extended experiments, we also consider **regularized logistic regression**, where a penalty term is added to the objective function:

[
\min_{\beta} ; -\ell(\beta)
;+; \lambda |\beta|_2^2  \quad \text{(Ridge)}
]

or

[
\min_{\beta} ; -\ell(\beta)
;+; \lambda |\beta|_1  \quad \text{(Lasso)}
]

Here, (\lambda) controls the strength of regularization:

* **large (\lambda)** → stronger shrinkage, simpler model
* **small (\lambda)** → coefficients closer to the unregularized model

These regularized GLMs can be fitted via `method = "glmnet"` in caret.

These hyperparameters are tuned in the third experiment using 10-fold cross-validation.


| Parameter | Meaning                         | Values tested              |
| --------- | ------------------------------- | -------------------------- |
| (\lambda) | Regularization strength (L1/L2) | 0, 0.001, 0.01, 0.1, 1, 10 |



## Method 2: Gaussian Naive Bayes (GNB)

### Model formulation
Gaussian Naive Bayes models the class-conditional distribution of each feature as Gaussian:
\[
p(x_j | y = c) = \frac{1}{\sqrt{2\pi\sigma_{jc}^2}} \exp\!\left(-\frac{(x_j - \mu_{jc})^2}{2\sigma_{jc}^2}\right),
\]
and applies Bayes’ rule assuming **conditional independence** between features:
\[
p(y|x) \propto p(y)\prod_{j=1}^d p(x_j|y).
\]
The predicted class is the one with the highest posterior probability.

### Theoretical properties and justification
Despite the strong independence assumption, Naive Bayes often performs surprisingly well in practice, especially when features are only weakly correlated. It is simple, fast, and provides a strong probabilistic baseline. GNB is particularly effective when the class distributions are approximately normal and separable in feature space.

Here is the improved version of the **Implementation details** section for Gaussian Naive Bayes, including an explanation of its hyperparameters and a clean table of values recommended for tuning.

### Implementation details 

Gaussian Naive Bayes (GNB) was implemented using `method = "nb"` from the **caret** package. Before model fitting, all predictors were **centered and scaled**. The classifier estimates, for each class, the mean and variance of every feature, assuming conditional independence among features.

Unlike logistic regression or SVM, GNB has very few tunable parameters. However, for improved robustness and calibration, **caret’s Naive Bayes implementation** allows controlling two useful hyperparameters:

* **Laplace smoothing (`laplace`)**
  Prevents zero-probability issues in categorical versions of NB; in Gaussian NB it has a mild stabilizing effect.

* **Kernel density estimation (`usekernel`)**
  If enabled, this replaces the Gaussian assumption with a non-parametric density estimate (useful when features deviate from normality).

These hyperparameters are tuned in the third experiment using 10-fold cross-validation.



### **Suggested hyperparameter grid**

| Parameter   | Meaning                                 | Values tested |
| ----------- | --------------------------------------- | ------------- |
| `laplace`   | Laplace smoothing (stability)           | 0, 0.5, 1     |
| `usekernel` | Kernel density instead of Gaussian      | TRUE, FALSE   |
| `adjust`    | Bandwidth multiplier for kernel density | 0.5, 1, 2     |




## Method 3: Support Vector Machine (SVM)

### Model formulation
A linear Support Vector Machine seeks the hyperplane \(w^\top x + b = 0\) that maximizes the margin between two classes:
\[
\min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(w^\top x_i + b)),
\]
where \(C > 0\) balances margin width and classification error (hinge loss). The prediction function is
\[
f(x) = \text{sign}(w^\top x + b).
\]

### Theoretical properties and justification
SVMs are **margin-based classifiers** that provide good generalization, especially in high-dimensional spaces. The hinge loss focuses on difficult cases (support vectors), making the classifier robust to noise and overfitting. A linear kernel is used here since the dataset is well-behaved and moderately dimensional.

### Implementation details 

Gaussian Naive Bayes (GNB) was implemented using `method = "nb"` from the **caret** package. Before model fitting, all predictors were **centered and scaled**. The classifier estimates, for each class, the mean and variance of every feature, assuming conditional independence among features.

Unlike logistic regression or SVM, GNB has very few tunable parameters. However, for improved robustness and calibration, **caret’s Naive Bayes implementation** allows controlling two useful hyperparameters:

* **Laplace smoothing (`laplace`)**
  Prevents zero-probability issues in categorical versions of NB; in Gaussian NB it has a mild stabilizing effect.

* **Kernel density estimation (`usekernel`)**
  If enabled, this replaces the Gaussian assumption with a non-parametric density estimate (useful when features deviate from normality).

These hyperparameters are tuned in the third experiment using 10-fold cross-validation.


| Parameter   | Meaning                                 | Values tested |
| ----------- | --------------------------------------- | ------------- |
| `laplace`   | Laplace smoothing (stability)           | 0, 0.5, 1     |
| `usekernel` | Kernel density instead of Gaussian      | TRUE, FALSE   |
| `adjust`    | Bandwidth multiplier for kernel density | 0.5, 1, 2     |




## Comparison framework

To determine the most effective approach, the performance of all models (GLM, GNB, SVM) across all three experimental setups (Baseline, PCA, Tuned) will be systematically compared.

The definitive evaluation will be based on model performance on the **unseen 20% holdout set**. This ensures a fair and unbiased assessment of generalization capability.



### Key Evaluation Metrics

The following standard classification metrics will be computed from the **holdout set predictions** for each model configuration:

* **Accuracy:** The overall proportion of correctly classified instances.
* **AUC-ROC:** (Area Under the Receiver Operating Characteristic Curve) Measures the model's ability to discriminate between the positive and negative classes, independent of the decision threshold.
* **Recall:** Measures the model's ability to identify all relevant positive cases (i.e., minimizing false negatives).
* **F1-Score:** The harmonic mean of Precision and Recall, providing a single score that balances both concerns, which is especially useful for imbalanced datasets.


### Analysis and Selection

The comparison will be structured to answer two primary questions by comparing the holdout set metrics:

1.  **Best Model:** Which classifier (**GLM, GNB, or SVM**) achieves the highest overall performance, particularly after its hyperparameters have been optimized in Experiment 3?
2.  **Best Representation:** Does the **PCA-transformed data** (Experiment 2) lead to better or worse generalization compared to the **original 20 features** (Experiment 1)?

Results from the 10-fold cross-validation (used during training) will also be examined to assess model **stability** and **robustness to data variations**. The final goal is to identify the single model (e.g., "Tuned SVM" or "Baseline GLM") that provides the best and most reliable predictive performance on new data.



# Discussion of results

## Detailed analysis

### Baseline

This chunk defines our custom evaluation function and sets up the data and cross-validation strategy.

```{r, warning=FALSE}
# --- 5. Custom Summary Function ---
# This function calculates all four metrics (Accuracy, AUC, Recall, F1)
# during cross-validation.
modelSummary <- function(data, lev, model) {
  
  # 1. Get Accuracy (from defaultSummary)
  #    Requires data$pred and data$obs
  acc_val <- defaultSummary(data, lev, model)["Accuracy"]
  
  # 2. Get AUC (ROC) and Recall (Sensitivity) (from twoClassSummary)
  #    Requires class probabilities (e.g., data$M and data$B)
  #    'lev[1]' is the "positive" class (which we set to "M")
  roc_stats <- twoClassSummary(data, lev, model)
  roc_val <- roc_stats["ROC"]
  recal_val <- roc_stats["Sens"] # 'Sens' is Recall
  
  # 3. Get F1-Score
  #    Requires data$pred and data$obs
  #    We must ensure the "positive" class is 'M' (Malignant)
  f1_val <- F1_Score(y_true = data$obs, 
                     y_pred = data$pred, 
                     positive = lev[1]) # lev[1] is "M"
  
  # 4. Name and return the vector of results
  c(Accuracy = acc_val, 
    ROC = roc_val, 
    Recall = recal_val,
    F1 = f1_val)
}



set.seed(123) # for reproducibility
train_index <- createDataPartition(data_clean_reduced$diagnosis, p = 0.80, list = FALSE)

training_data <- data_clean_reduced[train_index, ]
holdout_data  <- data_clean_reduced[-train_index, ] # For final testing later

# CRITICAL: Set "M" (Malignant) as the "positive" class
# This ensures Recall and F1 are calculated correctly.
training_data$diagnosis <- relevel(training_data$diagnosis, ref = "M")
holdout_data$diagnosis  <- relevel(holdout_data$diagnosis, ref = "M")

# --- 7. Cross-Validation Control ---
cv_control <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = modelSummary, # Use our custom function
  classProbs = TRUE,            # MUST be TRUE for AUC (ROC)
  savePredictions = "final",
  timingSamps = 50              # Samples for timing execution
)
```

### Experiment 1: Baseline Models



```{r, warning=FALSE}
# --- Experiment 1: Baseline Models (Reduced Features) ---
# We will store models in a list to compare them easily
models_baseline <- list()

# 1.1: Generalized Linear Model (GLM)
set.seed(123)
models_baseline$glm <- train(
  diagnosis ~ ., 
  data = training_data,
  method = "glm",
  family = "binomial", # Specifies logistic regression
  trControl = cv_control,
  metric = "ROC"      # Optimize for AUC-ROC
)

# 1.2: Gaussian Naive Bayes (GNB)
set.seed(123)
models_baseline$gnb <- train(
  diagnosis ~ ., 
  data = training_data,
  method = "nb",
  trControl = cv_control,
  metric = "ROC"
)

# 1.3: Linear Support Vector Machine (SVM)
set.seed(123)
models_baseline$svm <- train(
  diagnosis ~ ., 
  data = training_data,
  method = "svmLinear", # Using linear kernel
  trControl = cv_control,
  metric = "ROC"
)

# --- Collect and display baseline CV results ---
baseline_results <- resamples(models_baseline)
summary(baseline_results)

# --- Plot Baseline Comparison ---
dotplot(baseline_results, metric = "ROC.ROC")
dotplot(baseline_results, metric = "Recall.Sens")
dotplot(baseline_results, metric = "Accuracy.Accuracy")


# --- Execution Times ---
data.frame(
  Model = names(models_baseline),
  ExecTime_sec = sapply(models_baseline, function(x) x$times$everything["user.self"])
)
```



### Experiment 2: PCA

Here we run **Experiment 2: PCA Representation**. We add `preProcess = "pca"` to the `train()` call. This correctly applies PCA *within* the cross-validation folds, preventing data leakage.

*Note on PCA vs. LDA:* You asked whether to use PCA or LDA.

  * **PCA (Principal Component Analysis)** is **unsupervised**. It finds components that explain the most *variance* in the features, without looking at the `diagnosis` class.
  * **LDA (Linear Discriminant Analysis)** is **supervised**. It finds components that maximize the *separation* between the classes (`M` vs. `B`).
  * **Decision:** If your goal is just dimensionality reduction, PCA is standard. If your primary goal is class separation (which it is\!), **LDA is often a better choice for classification**. You can test it by replacing `"pca"` with `"lda"` in the `preProcess` argument.
  
To have an idea of the shape of our data in lower dimensional space we also plot the first two principal components colored by the target variable. By doing this we can observe which kind of separation exists between the two classes and determine different things. If we oberve linear separability we might conclude that linear models such as LDA migth perform well. If we observe that the classes don't follow a linear pattern we can suspect quadratic separations might work well. It may also be usefull to determine the best kernel function that can be applied to the data in order to train an SVM model.



```{r}
# data is already scaled in the preprocessing, if not we would need to scale ir before training PCA
pca = prcomp(training_data %>% dplyr::select(-diagnosis))
pca_data = data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], diagnosis = training_data$diagnosis)
ggplot(pca_data, aes(x = PC1, y = PC2, color = diagnosis)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "PCA of Training Data: First Two Principal Components",
       x = "Principal Component 1",
       y = "Principal Component 2") +
  theme_minimal() +
  geom_abline(slope = 0.5, intercept = 0.7, linetype = "dashed", color = "black",lwd = 0.8) +
  geom_function(fun = function(x) 0.8 + 0.6 * x - 0.1 * x^2,color = "brown",linetype = "dashed",lwd = 0.8) +
  scale_colour_manual(values = c("M" = "red", "B" = "blue"))
```

After the visualization, we realize that the data in 2 dimensions is separable for both linear and non linear decision boundaries, exceptiuating some overlapping points. We expect both LDA and QDA to perform well for this classification problem.

We did not observe complex patterns in the first 2 dimensions of our data after PCA transformation. If we were to train an SVM model we probably wouldn't need a kernel that caputurtes non-linear patterns such as RBF or spheric. Selecting a linear or polynomial kernel  with a moderate degree would probably be sufficient to separate the data correctly

Now we want to apply the different algorithms using PCA dimensions as input features. We select the number of dimensions that are able to accumulate 80% of the intertia of the data

```{r}
summary(pca)
```

We see that only selecting 5 principal components we are able to retain more than 80% of the variance of the data. Therefore we will select 5 components for our PCA transformation in the following models.




```{r exp2_pca, warning=FALSE}
# --- Experiment 2: PCA Representation ---
training_data_pca <- pca$x[, 1:5] %>% as.data.frame() %>% add_column(diagnosis = training_data$diagnosis)

models_pca <- list()

# 2.1: GLM + PCA
set.seed(123)

models_pca$glm <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "glm",
  family = "binomial",
  trControl = cv_control,
  metric = "ROC"
)


# 2.2: GNB + PCA
set.seed(123)
models_pca$gnb <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "nb",
  trControl = cv_control,
  metric = "ROC"
)

# 2.3: Linear SVM + PCA
set.seed(123)
models_pca$svm_linear <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "svmLinear",
  trControl = cv_control,
  metric = "ROC"
)

svm_grid <- data.frame(degree = 2, scale = 1, C = 1)
models_pca$svm_poly2 <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "svmPoly",
  trControl = cv_control,
  metric = "ROC",
  tuneGrid = svm_grid,
)


models_pca$lda <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "lda",
  trControl = cv_control,
  metric = "ROC",
)

models_pca$qda <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "qda",
  trControl = cv_control,
  metric = "ROC",
)


# --- Collect and display PCA CV results ---
pca_results <- resamples(models_pca)
summary(pca_results)

# --- Plot PCA Comparison ---
dotplot(pca_results, metric = "ROC.ROC")
dotplot(pca_results, metric = "Recall.Sens")
dotplot(pca_results, metric = "Accuracy.Accuracy")


# --- Execution Times ---
data.frame(
  Model = names(models_pca),
  ExecTime_sec = sapply(models_pca, function(x) x$times$everything["user.self"])
)
```
### Comparing Baseline vs PCA

```{r}
baseline_results_stat = (baseline_results[["values"]]) %>% dplyr::select(-Resample)
baseline_results_stat = as.data.frame(colMedians(as.matrix(baseline_results_stat))) %>% rename('Baseline' = 'colMedians(as.matrix(baseline_results_stat))')

pca_results_stat = (pca_results[["values"]]) %>% dplyr::select(-Resample)
pca_results_stat = as.data.frame(colMedians(as.matrix(pca_results_stat))) %>% rename('PCA' = 'colMedians(as.matrix(pca_results_stat))')

comparison = left_join(
  baseline_results_stat %>% rownames_to_column("Metric-Model"),
  pca_results_stat %>% rownames_to_column("Metric-Model"),
  by = "Metric-Model"
) %>%
  mutate(Difference = PCA - Baseline)
comparison
```

Similar performcance for both methods. PCA has slightly better median results than the baseline. Considering that we are reducing dimensionality and therefore complexity of the models we will conider training the models with PCA transformation in the next steps.


### Hyperparametrization

Here we run **Experiment 3: Model-specific Hyperparameter Tuning**.

#### GLM

We tune the **Regularized GLM (glmnet)**. This tunes `alpha` (L1/Lasso vs. L2/Ridge) and `lambda` (regularization strength).

```{r exp3_glm_tune, warning=FALSE}
# --- 3.1: GLM (glmnet) Tuning ---

# Create a grid of hyperparameters
glm_grid <- expand.grid(
  alpha = seq(0, 1, by = 0.5),      # 0=Ridge, 1=Lasso, 0.5=ElasticNet
  lambda = c(0.0001, 0.001, 0.01, 0.1, 1)
)

set.seed(123)
model_tuned_glm <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "glmnet",
  trControl = cv_control,
  tuneGrid = glm_grid,
  metric = "ROC"
)

plot(model_tuned_glm)
print(model_tuned_glm$bestTune)
print(paste("GLM Tune Time (sec):", model_tuned_glm$times$everything["user.self"]))
```
Additionaly to the regulkarization hyperparameters we tried to see the impact of the cutoff threshold on the recall and error rate (1-accuracy) of the model.





```{r}
holdout_data_pca <- predict(pca, newdata = holdout_data %>% dplyr::select(-diagnosis))[, 1:5] %>% as.data.frame() %>% add_column(diagnosis = holdout_data$diagnosis)


cutoffs = seq(0.2, 0.8, by = 0.05)
glm_probs <- predict(model_tuned_glm, newdata = holdout_data_pca, type = "prob")$M

glm_metrics <- data.frame(
  Cutoff = cutoffs,
  Recall = sapply(cutoffs, function(cut) {
    preds <- ifelse(glm_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "M")
    cm$byClass["Sensitivity"]
  }),

  Accuracy = sapply(cutoffs, function(cut) {
    preds <- ifelse(glm_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "M")
    cm$overall["Accuracy"]
  }
)
)


ggplot(glm_metrics, aes(x = Cutoff)) +
  geom_line(aes(y = Recall, color = "Recall"), size = 1) +
  geom_line(aes(y = 1 - Accuracy, color = "Error Rate"), size = 1) +
  labs(title = "GLM: Recall, F1-Score, and Accuracy vs. Decision Cutoff",
       x = "Decision Cutoff",
       y = "Metric Value") +
  theme_minimal() +
  ylim(0, 1)
```

Seeing that both metrics remain almos constant across different cutoff values indicates that the glm is gives very high predicted probabilities. This indicates taht the models works correctly and is very confident about its predictions.

We also tried to plot the ROC curve and analyze which of the threshold had a point (Recall, FPR) closest to (1,0) point, which would indicate the best trade-off between recall and false positive rate and therefore a great candidate for the cutoff threshold. Points around 0.5 scored with the exacct same distance from (1,0) point so we can select any of them without much difference in performance. We excluded this form the report since it did not lead to any relevant conclusion/imporvement.



#### GNB

Since we have numericall predicitve variables it does not make sense to tune the laplace hyperparameter which is used to avoid zero probabilities in categorical features. Just like in the previous section we will tune the threshold of the model to see its impact on recall and accuracy.


```{r exp3_gnb_tune, warning=FALSE}
# --- 3.2: GNB Tuning ---

# Create a grid based on your methodology


set.seed(123)
model_tuned_gnb <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "nb",
  kernel = FALSE,
  trControl = cv_control,
  metric = "ROC"
)

model_tuned_gnb[["results"]][1,4:7]
print(paste("GNB Tune Time (sec):", model_tuned_gnb$times$everything["user.self"]))
```


We now tune the threshold for the GNB model to see its impact on recall and accuracy.


```{r}
cutoffs = seq(0, 1, by = 0.05)
nb_probs <- predict(model_tuned_gnb, newdata = holdout_data_pca, type = "prob")$M

nb_metrics <- data.frame(
  Cutoff = cutoffs,
  Recall = sapply(cutoffs, function(cut) {
    preds <- ifelse(nb_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "B")
    cm$byClass["Sensitivity"]
  }),

  Accuracy = sapply(cutoffs, function(cut) {
    preds <- ifelse(nb_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "B")
    cm$overall["Accuracy"]
  }),

  Specificity = sapply(cutoffs, function(cut) {
    preds <- ifelse(nb_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "B")
    cm$byClass["Specificity"]
  }
),
  F1 = sapply(cutoffs, function(cut) {
    preds <- ifelse(nb_probs >= cut, "M", "B")
    cm <- confusionMatrix(factor(preds, levels = c("M", "B")), holdout_data$diagnosis, positive = "B")
    precision <- cm$byClass["Pos Pred Value"]
    recall <- cm$byClass["Sensitivity"]
    return(2 * (precision * recall) / (precision + recall))
    }
))

ggplot(nb_metrics, aes(x = Cutoff)) +
  geom_line(aes(y = Recall, color = "Recall"), size = 1) +
  geom_line(aes(y = 1 - Accuracy, color = "Error Rate"), size = 1) +
  labs(title = "NB: Recall, and Accuracy vs. Decision Cutoff",
       x = "Decision Cutoff",
       y = "Metric Value") +
  theme_minimal() +
  ylim(0, 1)
```

As we can see in the plot there exists an impact of the cutoff threshold on both recall and accuracy. Higher cutoffs lead to a decrease in performance.
To decide what the best cutoff is we can plot the ROC curve and determine which point is closest to (1,0) point. This point represents the best trade-off between recall and false positive rate.


```{r}
nb_metrics['distance'] = sqrt((1 - nb_metrics$Recall)^2 + (0 - (1 - nb_metrics$Specificity))^2)
optimal_cutoff_nb = nb_metrics %>% filter(distance == min(distance)) %>% dplyr::select(Cutoff, Recall, Specificity)
print(optimal_cutoff_nb)

# plot roc curve
ggplot(nb_metrics, aes(x = 1 - Specificity, y = Recall)) +
  geom_line(color = "blue", size = 1) +
  geom_point(aes(color = distance), size = 3) +
  labs(title = "GNB: ROC Curve with Distance to (1,0)",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Recall)") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  geom_point(data = optimal_cutoff_nb, aes(x = 1 - Specificity, y = Recall), color = "red", size = 6) +
  ylim(0,1) + 
  xlim(0,1) + 
  geom_text(aes(label = Cutoff, vjust = -1),check_overlap = T) +
  theme_minimal()
```

overwrite the default metrics by the ones obtained by tunning the threshold

```{r}
model_tuned_gnb[["results"]][["Accuracy.Accuracy"]] = nb_metrics[which(nb_metrics$Cutoff == optimal_cutoff_nb$Cutoff), "Accuracy"]
model_tuned_gnb[["results"]][["Recall.Sens"]] = nb_metrics[which(nb_metrics$Cutoff == optimal_cutoff_nb$Cutoff), "Recall"]
model_tuned_gnb[["results"]][["F1"]] = nb_metrics[which(nb_metrics$Cutoff == optimal_cutoff_nb$Cutoff), "F1"]
```


We find that the optimal threshold is 0.3 giving the best tade-off between recall and specificity. This value also gives the best accuracy.
The recall for this cutoff is `r round(optimal_cutoff_nb$Recall,3)`, the specificity is `r round(optimal_cutoff_nb$Specificity,3)` and the accuracy is `r round(optimal_cutoff_nb$Accuracy,3)`.


#### SVM

##### Linear 

We tune the **Linear SVM**, focusing on the cost parameter `C`, which controls the trade-off between margin width and misclassification.

```{r exp3_svm_tune, warning=FALSE}
# --- 3.3: SVM (Linear) Tuning ---

# Create a grid for the Cost (C) parameter
svm_grid <- expand.grid(
  C = c(0.01, 0.1, 1,2,5, 10)
)

set.seed(123)
model_tuned_svm_lin <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "svmLinear",
  trControl = cv_control,
  tuneGrid = svm_grid,
  metric = "ROC"
)

plot(model_tuned_svm_lin)
print(model_tuned_svm_lin$bestTune)
print(paste("SVM Tune Time (sec):", model_tuned_svm_lin$times$everything["user.self"]))
```
```{r exp3_svm_tune, warning=FALSE}
svm_grid <- expand.grid(
  degree = c(2, 3),       # polynomial order
  scale  = c(0.01, 0.1, 1),  # gamma-like scaling factor
  C = c(0.01, 0.1, 1,2,5, 10)
)

set.seed(123)
model_tuned_svm_poly <- train(
  diagnosis ~ ., 
  data = training_data_pca,
  method = "svmPoly",
  trControl = cv_control,
  tuneGrid = svm_grid,
  metric = "ROC"
)


plot(model_tuned_svm_poly)
print(model_tuned_svm_poly$bestTune)
print(paste("SVM Tune Time (sec):", model_tuned_svm_poly$times$everything["user.self"]))
```
## Overall Performance

This section combines all 9 trained models (3x Baseline, 3x PCA, 3x Tuned) to find the single best-performing model based on cross-validation results.

```{r overall_comparison, warning=FALSE}
# --- Collect all 9 models into one list ---
all_models_list <- list(
  Base_GLM = models_baseline$glm,
  Base_GNB = models_baseline$gnb,
  Base_SVM = models_baseline$svm,
  PCA_GLM  = models_pca$glm,
  PCA_GNB  = models_pca$gnb,
  PCA_SVM_LIN  = models_pca$svm_linear,
  PCA_SVM_POLY  = models_pca$svm_poly,
  Tune_GLM = model_tuned_glm,
  Tune_GNB = model_tuned_gnb,
  Tune_SVM_Lin = model_tuned_svm_lin,
  Tune_SVM_Poly = model_tuned_svm_poly
)




df <- do.call(rbind, lapply(seq_along(all_models_list), function(i) {
  
  model_name <- names(all_models_list)[i]
  model <- all_models_list[[i]]
  best_row <- model$results[which.max(model$results$ROC.ROC), , drop = FALSE]
  data.frame(
    Model   = model_name,
    Accuracy = best_row[["Accuracy.Accuracy"]],
    ROC      = best_row[["ROC.ROC"]],
    Recall   = best_row[["Recall.Sens"]],
    F1       = best_row[["F1"]],
    Execution_time = model$times$everything["elapsed"]
  )
}))


```

```{r}
df['log_time_100r'] = log1p( 100* (df$Execution_time / nrow(training_data_pca)))
w = 0.9
df$score = w * df$ROC + (1 - w) * (1 - df$log_time_100r)
```


```{r}
ggplot(df, aes(x = ROC, y = Execution_time, color = Model)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_text(aes(label = Model), vjust = -1.2, size = 3.5) + 
  labs(
    title = "Model Performance vs. Training Time Trade-off",
    subtitle = "Best models are in the top-left corner (High Accuracy, Low Time)",
    x = "Mean Accuracy (10-fold CV)",
    y = "Mean Training Time (seconds)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```




## Model selection and generalization error

Finally, we select the **single best model** from the `all_results` comparison above (e.g., the one with the highest mean `ROC` or `Recall`) and test it on the **unseen 20% holdout set** to measure its true generalization error.

```{r final_holdout_test, warning=FALSE}
# --- 8. Model Selection and Generalization Error ---

# Step 1: Select the best model.
# By looking at the plots, we choose our winner.
# FOR THIS EXAMPLE, let's assume 'Tune_SVM' (model_tuned_svm) was the best.
# You MUST change this line to reflect your actual best model.

final_model <- model_tuned_svm_poly 
print(paste("Selected Final Model:", final_model$method, "with params:", 
            paste(names(final_model$bestTune), final_model$bestTune, collapse=", ")))


# Step 2: Test on the 20% holdout set
# Get class predictions
holdout_pred <- predict(final_model, newdata = holdout_data_pca)

# Get class probabilities (for AUC)
holdout_probs <- predict(final_model, newdata = holdout_data_pca, type = "prob")


# Step 3: Calculate final metrics
print("--- Holdout Set Performance (Generalization Error) ---")

# A) Get Confusion Matrix, Accuracy, Recall, F1
# We explicitly set the positive class to "M"
cm <- confusionMatrix(
  data = holdout_pred, 
  reference = holdout_data$diagnosis,
  positive = "M" 
)
print(cm$table) # The confusion matrix


# B) Calculate AUC-ROC
# We need the probabilities for the *positive* class, "M"
roc_obj <- pROC::roc(
  response = holdout_data$diagnosis, 
  predictor = holdout_probs$M,
  levels = c("B", "M") # Specify levels, B=control, M=case
)
auc_val <- pROC::auc(roc_obj)


# C) Collect and display all final metrics in a clean table
final_metrics <- data.frame(
  Metric = c("Accuracy", "Recall (Sensitivity)", "F1-Score", "AUC-ROC"),
  Value = c(
    cm$overall["Accuracy"],
    cm$byClass["Sensitivity"], # This is Recall
    cm$byClass["F1"],
    auc_val
  )
)

print(final_metrics)
```




# Conclusions











# References
[1]: https://www.nepjol.info/index.php/nccsrj/article/view/75469?utm_source=chatgpt.com "Breast Cancer Prediction: A Comparative Study of Support Vector Machine and Logistic Regression | National College of Computer Studies Research Journal"
[2]: https://arxiv.org/abs/2306.02449?utm_source=chatgpt.com "The Power Of Simplicity: Why Simple Linear Models Outperform Complex Machine Learning Techniques -- Case Of Breast Cancer Diagnosis"
[3]: https://beei.org/index.php/EEI/article/view/4448/0?utm_source=chatgpt.com "Breast cancer detection: an effective comparison of different machine learning algorithms on the Wisconsin dataset | Hossin | Bulletin of Electrical Engineering and Informatics"

  Wolberg, W., Mangasarian, O., Street, N., & Street, W. (1993). Breast Cancer Wisconsin (Diagnostic) [Dataset]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5DW2B>.

#A Additional results [Your additional results here if needed]

#B Mathematical derivations [Your derivations here if needed]

#C Implementation details [Your implementation details here if needed






# Discussion of results
## Overall performance
[Your overall results here]

## Detailed Analysis
[Your detailed analysis here with insightful interpretation]

## Model selection and generalization error
[Your final model selection and generalization error estimate here]


## Final results
#TODO Adapt it to the schema provided by the teacher




## Accuracy vs. Training Time Trade-off

```{r}
# --- 9. ROC vs. Training Time Trade-off ---

# We need 'tibble' to use rownames_to_column
library(tibble) 

# --- Process Accuracy Data ---
# This part was correct, as '$values' *is* per-fold data
accuracy_stats <- all_model_results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = "ModelMetric",
    values_to = "Value"
  ) %>%
  tidyr::separate(ModelMetric, into = c("Model", "Metric"), sep = "~") %>%
  filter(Metric == "ROC.ROC") %>%
  group_by(Model) %>%
  summarize(Mean_ROC = mean(Value, na.rm = TRUE))


# --- Process Timings Data (This is the FIX) ---
# As your screenshot shows, '$timings' is a SUMMARY table.
# The 'Model' is in the row names, and 'Everything' is the total time.
time_stats <- all_model_results$timings %>%
  
  # 1. Convert row names (e.g., "GLM_Reduced") into a real column
  rownames_to_column(var = "Model") %>%
  
  # 2. Select the Model and the 'Everything' column (total time)
  #    and rename 'Everything' to match our other table.
  select(Model, Mean_Time_sec = Everything)


# --- Join and Plot ---
# This join will now work because both tables have a 'Model' column
performance_summary <- inner_join(accuracy_stats, time_stats, by = "Model")

# 4. Print the final summary table
cat("--- Final Performance vs. Cost Summary ---\n")
performance_summary <- performance_summary %>%
  arrange(desc(Mean_ROC))
print(performance_summary)
```


```{r}
# 5. Plot the Trade-off
ggplot(performance_summary, aes(x = Mean_ROC, y = Mean_Time_sec, color = Model)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_text(aes(label = Model), vjust = -1.2, size = 3.5) + 
  labs(
    title = "Model Performance vs. Training Time Trade-off",
    subtitle = "Best models are in the top-left corner (High Accuracy, Low Time)",
    x = "Mean Accuracy (10-fold CV)",
    y = "Mean Training Time (seconds)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```



```{r}
library(tibble)
library(knitr)
library(kableExtra)
library(dplyr)
library(tidyr)

# --- 1. Get Mean and SD for all metrics ---
metric_stats <- all_model_results$values %>%
  pivot_longer(
    cols = -Resample,
    names_to = "ModelMetric",
    values_to = "Value"
  ) %>%
  tidyr::separate(ModelMetric, into = c("Model", "Metric"), sep = "~") %>%
  mutate(Metric = str_replace(Metric, "\\.ROC$", "")) %>%
  mutate(Metric = str_replace(Metric, "\\.Accuracy$", "")) %>%
  # Filter for only the metrics we want in the table
  filter(Metric %in% c("Accuracy", "F1", "ROC")) %>% 
  
  # Calculate Mean and SD for each
  group_by(Model, Metric) %>%
  summarize(
    Mean = mean(Value, na.rm = TRUE),
    SD = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  
  # Pivot wider so each metric has a Mean and SD column
  # e.g., Mean_Accuracy, SD_Accuracy, Mean_F1, SD_F1
  pivot_wider(
    names_from = Metric,
    values_from = c(Mean, SD)
  )

# --- 2. Get Training Time ---
time_stats <- all_model_results$timings %>%
  rownames_to_column(var = "Model") %>%
  select(Model, Mean_Time_sec = Everything)

# --- 3. Join, Format, and Bold ---
final_table_data <- inner_join(metric_stats, time_stats, by = "Model") %>%
  
  # Create the "Mean ± SD" strings, rounding to 2 decimal places
  mutate(
    `Method` = Model, # Rename for the table
    `Accuracy` = paste0(format(round(Mean_Accuracy, 2), nsmall = 2), " ± ", format(round(SD_Accuracy, 2), nsmall = 2)),
    `F1-score` = paste0(format(round(Mean_F1, 2), nsmall = 2), " ± ", format(round(SD_F1, 2), nsmall = 2)),
    `AUC` = paste0(format(round(Mean_ROC, 2), nsmall = 2), " ± ", format(round(SD_ROC, 2), nsmall = 2)),
    `Training time (s)` = round(Mean_Time_sec, 3)
  ) %>%
  
  # --- THIS IS THE LINE THAT IS REMOVED ---
  # select(Method, Accuracy, `F1-score`, AUC, `Training time (s)`) %>%

  # --- 4. Add Bolding for Best Performance ---
  # This mutate call now works because Mean_Accuracy, Mean_F1, etc. still exist
  mutate(
    Accuracy = ifelse(Mean_Accuracy == max(Mean_Accuracy, na.rm = TRUE), cell_spec(Accuracy, bold = T), Accuracy),
    `F1-score` = ifelse(Mean_F1 == max(Mean_F1, na.rm = TRUE), cell_spec(`F1-score`, bold = T), `F1-score`),
    AUC = ifelse(Mean_ROC == max(Mean_ROC, na.rm = TRUE), cell_spec(AUC, bold = T), AUC)
  )

# --- 5. Print the kable table ---
# `escape = F` is required to render the bold HTML tags
# THIS select() call is the correct one, as it's at the end.
final_table_data %>%
  select(Method, Accuracy, `F1-score`, AUC, `Training time (s)`) %>%
  kable(format = "html", escape = F, align = "c") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F) %>%
  add_header_above(c(" " = 1, "Performance Metrics" = 3, "Cost" = 1))
```